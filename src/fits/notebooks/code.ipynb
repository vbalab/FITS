{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7de60f51",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from IPython.display import clear_output\n",
    "\n",
    "\n",
    "notebook_dir = \"/home/balabaevvl/courses/project/FITS/src/\"\n",
    "os.chdir(notebook_dir)\n",
    "\n",
    "GPUs = [\n",
    "    \"GPU-e83bd31b-fcb9-b8de-f617-2d717619413b\",\n",
    "    \"GPU-5a9b7750-9f85-49a5-3aae-fe07b1b7661d\",\n",
    "    \"GPU-fe2d8dfd-06f2-a5c4-a7fd-4a5f23947005\",\n",
    "    \"GPU-0c320096-21ee-4060-8731-826ca2febfab\",\n",
    "    \"GPU-baef952c-6609-aace-3b78-e4e07788d5de\",\n",
    "    \"GPU-3979d65b-c238-4e9c-0c1c-1aa3f05c56a1\",\n",
    "    \"GPU-6c76a2c5-5375-aa06-11d4-0fddfac30e91\",\n",
    "]\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = f\"{GPUs[1]}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e0d30bc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from fits.data.download import DownloadDatasetAirQuality\n",
    "\n",
    "DownloadDatasetAirQuality()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d642c530",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# from fits.config import DatasetsPaths\n",
    "\n",
    "# df = pd.read_csv(\n",
    "#     DatasetsPaths.pm25.value,\n",
    "#     index_col=\"datetime\",\n",
    "#     parse_dates=True,\n",
    "# )\n",
    "\n",
    "# df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dd541250",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from fits.data.dataset import ModelMode, DatasetAirQuality\n",
    "\n",
    "# dataset = DatasetAirQuality(ModelMode.train)\n",
    "\n",
    "# for sample in dataset:\n",
    "#     break\n",
    "# sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6db29570",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from fits.modelling.framework import Train, Evaluate\n",
    "from fits.data.dataset import DatasetAirQuality\n",
    "from fits.data.dataloader import ForecastingDataLoader\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "train_loader, valid_loader, test_loader = ForecastingDataLoader(\n",
    "    DatasetAirQuality, batch_size=128\n",
    ")\n",
    "normalization_stats = train_loader.dataset.normalization_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "351e9051",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import pickle\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "def visualize_forecast_sample(\n",
    "    evaluation_dir: str | Path,\n",
    "    nsample: int = 10,\n",
    "    n_features: int = 36,\n",
    "    sample_index: int = 0,\n",
    "    ncols: int = 4,\n",
    "    figsize=(24, 36),\n",
    ") -> None:\n",
    "    \"\"\"\n",
    "    Plot a separate subplot for each feature in feature_index.\n",
    "    \"\"\"\n",
    "    evaluation_dir = Path(evaluation_dir)\n",
    "    generated_path = evaluation_dir / f\"generated_outputs_nsample{nsample}.pk\"\n",
    "\n",
    "    with open(generated_path, \"rb\") as f:\n",
    "        (\n",
    "            forecasted_data,\n",
    "            forecast_mask,\n",
    "            observed_data,\n",
    "            observed_mask,\n",
    "            time_points,\n",
    "            scaler_tensor,\n",
    "            mean_tensor,\n",
    "        ) = pickle.load(f)\n",
    "\n",
    "    forecasted_data = forecasted_data.cpu()\n",
    "    forecast_mask = forecast_mask.cpu()\n",
    "    observed_data = observed_data.cpu()\n",
    "    observed_mask = observed_mask.cpu()\n",
    "    time_points = time_points.cpu()\n",
    "\n",
    "    time_axis = time_points[sample_index].numpy()\n",
    "\n",
    "    ncols = min(ncols, n_features)\n",
    "    nrows = math.ceil(n_features / ncols)\n",
    "\n",
    "    fig, axes = plt.subplots(nrows=nrows, ncols=ncols, figsize=figsize)\n",
    "    axes = axes.flatten() if n_features > 1 else [axes]\n",
    "\n",
    "    # ---- LOOP OVER FEATURES ----\n",
    "    for ax, feat in zip(axes, range(n_features)):\n",
    "        forecast_samples = forecasted_data[sample_index, :, :, feat]\n",
    "        sample_mask = forecast_mask[sample_index, :, feat].bool()\n",
    "        observed_series = observed_data[sample_index, :, feat]\n",
    "        observed_series_mask = observed_mask[sample_index, :, feat].bool()\n",
    "\n",
    "        # Median + intervals\n",
    "        median = forecast_samples.median(dim=0).values\n",
    "        lower, upper = torch.quantile(forecast_samples, torch.tensor([0.1, 0.9]), dim=0)\n",
    "\n",
    "        # Mask missing\n",
    "        median = median.masked_fill(~sample_mask, torch.nan)\n",
    "        lower = lower.masked_fill(~sample_mask, torch.nan)\n",
    "        upper = upper.masked_fill(~sample_mask, torch.nan)\n",
    "\n",
    "        # Convert\n",
    "        obs_mask_np = observed_series_mask.numpy()\n",
    "        obs_series_np = observed_series.numpy()\n",
    "        sample_mask_np = sample_mask.numpy()\n",
    "        median_np = median.numpy()\n",
    "        lower_np = lower.numpy()\n",
    "        upper_np = upper.numpy()\n",
    "\n",
    "        # ---- PLOTTING INTO ax ----\n",
    "        ax.scatter(\n",
    "            time_axis[obs_mask_np],\n",
    "            obs_series_np[obs_mask_np],\n",
    "            color=\"black\",\n",
    "            s=10,\n",
    "            label=\"Observed\",\n",
    "        )\n",
    "        ax.plot(time_axis, median_np, label=\"Median\", color=\"tab:green\")\n",
    "        ax.fill_between(\n",
    "            time_axis,\n",
    "            lower_np,\n",
    "            upper_np,\n",
    "            where=sample_mask_np,\n",
    "            alpha=0.3,\n",
    "            color=\"tab:green\",\n",
    "            label=\"10â€“90%\",\n",
    "        )\n",
    "\n",
    "        ax.set_title(f\"Feature {feat}\")\n",
    "        ax.set_xlabel(\"Time step\")\n",
    "        ax.set_ylabel(\"Value\")\n",
    "        ax.grid(True)\n",
    "        ax.legend()\n",
    "\n",
    "    # Turn off unused axes if any\n",
    "    for ax in axes[n_features:]:\n",
    "        ax.axis(\"off\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d9f255f",
   "metadata": {},
   "source": [
    "# CSDI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f4e2d0bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from fits.modelling.CSDI.adapter import CSDIAdapter\n",
    "\n",
    "\n",
    "csdi = CSDIAdapter().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9eb28f22",
   "metadata": {},
   "outputs": [],
   "source": [
    "Train(csdi, train_loader, valid_loader, epochs=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4020462",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_foldername = \"\"\n",
    "\n",
    "# state = torch.load(\n",
    "#     f\"../data/models/training/{model_foldername}/model.pth\",\n",
    "#     map_location=device,\n",
    "# )\n",
    "\n",
    "# csdi.load_state_dict(state)\n",
    "# csdi.to(device)\n",
    "\n",
    "# clear_output()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f7eec74",
   "metadata": {},
   "outputs": [],
   "source": [
    "Evaluate(csdi, test_loader, normalization_stats, nsample=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a0411fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# eval_foldername = \"\"\n",
    "# visualize_forecast_sample(f\"../data/models/evaluation/{eval_foldername}\", nsample=10, sample_index=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1078b24a",
   "metadata": {},
   "source": [
    "# DiffusionTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8418a38",
   "metadata": {},
   "outputs": [],
   "source": [
    "from fits.modelling.DiffusionTS.adapter import DiffusionTSAdapter\n",
    "\n",
    "\n",
    "diffusionts = DiffusionTSAdapter().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "482cde9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "Train(diffusionts, train_loader, valid_loader, epochs=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd235397",
   "metadata": {},
   "outputs": [],
   "source": [
    "Evaluate(diffusionts, test_loader, normalization_stats, nsample=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1cd3337",
   "metadata": {},
   "outputs": [],
   "source": [
    "# eval_foldername = \"\"\n",
    "# visualize_forecast_sample(f\"../data/models/evaluation/{eval_foldername}\", nsample=10, sample_index=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c2c7c3d",
   "metadata": {},
   "source": [
    "# FM-TS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3d14adaf",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "unsupported operand type(s) for %: 'NoneType' and 'int'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 4\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mfits\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmodelling\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mFMTS\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01madapter\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m FMTSAdapter\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m fmts = \u001b[43mFMTSAdapter\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m.to(device)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/courses/project/FITS/src/fits/modelling/FMTS/adapter.py:48\u001b[39m, in \u001b[36mFMTSAdapter.__init__\u001b[39m\u001b[34m(self, config)\u001b[39m\n\u001b[32m     46\u001b[39m \u001b[38;5;28msuper\u001b[39m().\u001b[34m__init__\u001b[39m(config)\n\u001b[32m     47\u001b[39m \u001b[38;5;28mself\u001b[39m.config: FMTSConfig = config\n\u001b[32m---> \u001b[39m\u001b[32m48\u001b[39m \u001b[38;5;28mself\u001b[39m.fmts = \u001b[43mFM_TS\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfmts_kwargs\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m.to(\u001b[38;5;28mself\u001b[39m.device)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/courses/project/FITS/src/fits/modelling/FMTS/interpretable_diffusion/FMTS.py:35\u001b[39m, in \u001b[36mFM_TS.__init__\u001b[39m\u001b[34m(self, seq_length, feature_size, n_layer_enc, n_layer_dec, d_model, n_heads, mlp_hidden_times, attn_pd, resid_pd, kernel_size, padding_size, **kwargs)\u001b[39m\n\u001b[32m     32\u001b[39m \u001b[38;5;28mself\u001b[39m.seq_length = seq_length\n\u001b[32m     33\u001b[39m \u001b[38;5;28mself\u001b[39m.feature_size = feature_size\n\u001b[32m---> \u001b[39m\u001b[32m35\u001b[39m \u001b[38;5;28mself\u001b[39m.model = \u001b[43mTransformer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn_feat\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfeature_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_channel\u001b[49m\u001b[43m=\u001b[49m\u001b[43mseq_length\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_layer_enc\u001b[49m\u001b[43m=\u001b[49m\u001b[43mn_layer_enc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_layer_dec\u001b[49m\u001b[43m=\u001b[49m\u001b[43mn_layer_dec\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     36\u001b[39m \u001b[43m                         \u001b[49m\u001b[43mn_heads\u001b[49m\u001b[43m=\u001b[49m\u001b[43mn_heads\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattn_pdrop\u001b[49m\u001b[43m=\u001b[49m\u001b[43mattn_pd\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mresid_pdrop\u001b[49m\u001b[43m=\u001b[49m\u001b[43mresid_pd\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmlp_hidden_times\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmlp_hidden_times\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     37\u001b[39m \u001b[43m                         \u001b[49m\u001b[43mmax_len\u001b[49m\u001b[43m=\u001b[49m\u001b[43mseq_length\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_embd\u001b[49m\u001b[43m=\u001b[49m\u001b[43md_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconv_params\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\u001b[43mkernel_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpadding_size\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     40\u001b[39m \u001b[38;5;28mself\u001b[39m.alpha = \u001b[32m3\u001b[39m  \u001b[38;5;66;03m## t shifting, change to 1 is the uniform sampling during inference\u001b[39;00m\n\u001b[32m     41\u001b[39m \u001b[38;5;28mself\u001b[39m.time_scalar = \u001b[32m1000\u001b[39m \u001b[38;5;66;03m## scale 0-1 to 0-1000 for time embedding\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/courses/project/FITS/src/fits/modelling/FMTS/interpretable_diffusion/transformer.py:525\u001b[39m, in \u001b[36mTransformer.__init__\u001b[39m\u001b[34m(self, n_feat, n_channel, n_layer_enc, n_layer_dec, n_embd, n_heads, attn_pdrop, resid_pdrop, mlp_hidden_times, block_activate, max_len, conv_params, **kwargs)\u001b[39m\n\u001b[32m    508\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__init__\u001b[39m(\n\u001b[32m    509\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m    510\u001b[39m     n_feat,\n\u001b[32m   (...)\u001b[39m\u001b[32m    522\u001b[39m     **kwargs\n\u001b[32m    523\u001b[39m ):\n\u001b[32m    524\u001b[39m     \u001b[38;5;28msuper\u001b[39m().\u001b[34m__init__\u001b[39m()\n\u001b[32m--> \u001b[39m\u001b[32m525\u001b[39m     \u001b[38;5;28mself\u001b[39m.emb = \u001b[43mConv_MLP\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn_feat\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_embd\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mresid_pdrop\u001b[49m\u001b[43m=\u001b[49m\u001b[43mresid_pdrop\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    526\u001b[39m     \u001b[38;5;28mself\u001b[39m.inverse = Conv_MLP(n_embd, n_feat, resid_pdrop=resid_pdrop)\n\u001b[32m    528\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m conv_params \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m conv_params[\u001b[32m0\u001b[39m] \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/courses/project/FITS/src/fits/modelling/FMTS/interpretable_diffusion/model_utils.py:157\u001b[39m, in \u001b[36mConv_MLP.__init__\u001b[39m\u001b[34m(self, in_dim, out_dim, resid_pdrop)\u001b[39m\n\u001b[32m    152\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, in_dim, out_dim, resid_pdrop=\u001b[32m0.\u001b[39m):\n\u001b[32m    153\u001b[39m     \u001b[38;5;28msuper\u001b[39m().\u001b[34m__init__\u001b[39m()\n\u001b[32m    154\u001b[39m     \u001b[38;5;28mself\u001b[39m.sequential = nn.Sequential(\n\u001b[32m    155\u001b[39m         Transpose(shape=(\u001b[32m1\u001b[39m, \u001b[32m2\u001b[39m)),\n\u001b[32m    156\u001b[39m         \u001b[38;5;66;03m# nn.Conv1d(in_dim, out_dim, 3, stride=1, padding=1),\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m157\u001b[39m         \u001b[43mnn\u001b[49m\u001b[43m.\u001b[49m\u001b[43mConv1d\u001b[49m\u001b[43m(\u001b[49m\u001b[43min_dim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout_dim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkernel_size\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m3\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstride\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpadding\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m)\u001b[49m,  \u001b[38;5;66;03m## hucfg925\u001b[39;00m\n\u001b[32m    158\u001b[39m         \u001b[38;5;66;03m# nn.Conv1d(in_dim, out_dim, kernel_size=5, stride=1, padding=2),\u001b[39;00m\n\u001b[32m    159\u001b[39m         nn.Dropout(p=resid_pdrop),\n\u001b[32m    160\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/courses/venv311/lib/python3.11/site-packages/torch/nn/modules/conv.py:338\u001b[39m, in \u001b[36mConv1d.__init__\u001b[39m\u001b[34m(self, in_channels, out_channels, kernel_size, stride, padding, dilation, groups, bias, padding_mode, device, dtype)\u001b[39m\n\u001b[32m    336\u001b[39m padding_ = padding \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(padding, \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m _single(padding)\n\u001b[32m    337\u001b[39m dilation_ = _single(dilation)\n\u001b[32m--> \u001b[39m\u001b[32m338\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[34;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[32m    339\u001b[39m \u001b[43m    \u001b[49m\u001b[43min_channels\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    340\u001b[39m \u001b[43m    \u001b[49m\u001b[43mout_channels\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    341\u001b[39m \u001b[43m    \u001b[49m\u001b[43mkernel_size_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    342\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstride_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    343\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpadding_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    344\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdilation_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    345\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    346\u001b[39m \u001b[43m    \u001b[49m\u001b[43m_single\u001b[49m\u001b[43m(\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    347\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgroups\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    348\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    349\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpadding_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    350\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mfactory_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    351\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/courses/venv311/lib/python3.11/site-packages/torch/nn/modules/conv.py:109\u001b[39m, in \u001b[36m_ConvNd.__init__\u001b[39m\u001b[34m(self, in_channels, out_channels, kernel_size, stride, padding, dilation, transposed, output_padding, groups, bias, padding_mode, device, dtype)\u001b[39m\n\u001b[32m    107\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m in_channels % groups != \u001b[32m0\u001b[39m:\n\u001b[32m    108\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33min_channels must be divisible by groups\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m109\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mout_channels\u001b[49m\u001b[43m \u001b[49m\u001b[43m%\u001b[49m\u001b[43m \u001b[49m\u001b[43mgroups\u001b[49m != \u001b[32m0\u001b[39m:\n\u001b[32m    110\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mout_channels must be divisible by groups\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    111\u001b[39m valid_padding_strings = {\u001b[33m\"\u001b[39m\u001b[33msame\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mvalid\u001b[39m\u001b[33m\"\u001b[39m}\n",
      "\u001b[31mTypeError\u001b[39m: unsupported operand type(s) for %: 'NoneType' and 'int'"
     ]
    }
   ],
   "source": [
    "from fits.modelling.FMTS.adapter import FMTSAdapter\n",
    "\n",
    "\n",
    "fmts = FMTSAdapter().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8e84eba",
   "metadata": {},
   "outputs": [],
   "source": [
    "Train(fmts, train_loader, valid_loader, epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c790008c",
   "metadata": {},
   "outputs": [],
   "source": [
    "Evaluate(fmts, test_loader, normalization_stats, nsample=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b01963e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Comparison of truth VS forecasted in:\n",
    "# PSA plot\n",
    "# t-SNE plot\n",
    "# Data Density"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e93a485d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6b19bf4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eea6c9af",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
