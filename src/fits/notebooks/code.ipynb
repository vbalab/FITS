{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7de60f51",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import os\n",
    "from IPython.display import clear_output\n",
    "\n",
    "project_root = next(\n",
    "    (candidate for candidate in [Path.cwd()] + list(Path.cwd().parents) if (candidate / \"pyproject.toml\").exists()),\n",
    "    Path.cwd(),\n",
    ")\n",
    "os.chdir(project_root)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0d30bc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from fits.data.download import DownloadDatasetAirQuality\n",
    "\n",
    "DownloadDatasetAirQuality()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d642c530",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# from fits.config import DatasetsPaths\n",
    "\n",
    "# df = pd.read_csv(\n",
    "#     DatasetsPaths.pm25.value,\n",
    "#     index_col=\"datetime\",\n",
    "#     parse_dates=True,\n",
    "# )\n",
    "\n",
    "# df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd541250",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from fits.data.dataset import ModelMode, DatasetAirQuality\n",
    "\n",
    "# dataset = DatasetAirQuality(ModelMode.train)\n",
    "\n",
    "# for sample in dataset:\n",
    "#     break\n",
    "# sample\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6db29570",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from fits.modelling.framework import Train, Evaluate\n",
    "from fits.data.dataset import DatasetAirQuality\n",
    "from fits.data.dataloader import ForecastingDataLoader\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "train_loader, valid_loader, test_loader = ForecastingDataLoader(\n",
    "    DatasetAirQuality, batch_size=128\n",
    ")\n",
    "normalization_stats = train_loader.dataset.normalization_stats\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "351e9051",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import pickle\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "def visualize_forecast_sample(\n",
    "    evaluation_dir: str | Path,\n",
    "    nsample: int = 10,\n",
    "    n_features: int = 36,\n",
    "    sample_index: int = 0,\n",
    "    ncols: int = 4,\n",
    "    figsize=(24, 36),\n",
    ") -> None:\n",
    "    \"\"\"\n",
    "    Plot a separate subplot for each feature in feature_index.\n",
    "    \"\"\"\n",
    "    evaluation_dir = Path(evaluation_dir)\n",
    "    generated_path = evaluation_dir / f\"generated_outputs_nsample{nsample}.pk\"\n",
    "\n",
    "    with open(generated_path, \"rb\") as f:\n",
    "        (\n",
    "            forecasted_data,\n",
    "            forecast_mask,\n",
    "            observed_data,\n",
    "            observed_mask,\n",
    "            time_points,\n",
    "            scaler_tensor,\n",
    "            mean_tensor,\n",
    "        ) = pickle.load(f)\n",
    "\n",
    "    forecasted_data = forecasted_data.cpu()\n",
    "    forecast_mask = forecast_mask.cpu()\n",
    "    observed_data = observed_data.cpu()\n",
    "    observed_mask = observed_mask.cpu()\n",
    "    time_points = time_points.cpu()\n",
    "\n",
    "    time_axis = time_points[sample_index].numpy()\n",
    "\n",
    "    ncols = min(ncols, n_features)\n",
    "    nrows = math.ceil(n_features / ncols)\n",
    "\n",
    "    fig, axes = plt.subplots(nrows=nrows, ncols=ncols, figsize=figsize)\n",
    "    axes = axes.flatten() if n_features > 1 else [axes]\n",
    "\n",
    "    # ---- LOOP OVER FEATURES ----\n",
    "    for ax, feat in zip(axes, range(n_features)):\n",
    "        forecast_samples = forecasted_data[sample_index, :, :, feat]\n",
    "        sample_mask = forecast_mask[sample_index, :, feat].bool()\n",
    "        observed_series = observed_data[sample_index, :, feat]\n",
    "        observed_series_mask = observed_mask[sample_index, :, feat].bool()\n",
    "\n",
    "        # Median + intervals\n",
    "        median = forecast_samples.median(dim=0).values\n",
    "        lower, upper = torch.quantile(forecast_samples, torch.tensor([0.1, 0.9]), dim=0)\n",
    "\n",
    "        # Mask missing\n",
    "        median = median.masked_fill(~sample_mask, torch.nan)\n",
    "        lower = lower.masked_fill(~sample_mask, torch.nan)\n",
    "        upper = upper.masked_fill(~sample_mask, torch.nan)\n",
    "\n",
    "        # Convert\n",
    "        obs_mask_np = observed_series_mask.numpy()\n",
    "        obs_series_np = observed_series.numpy()\n",
    "        sample_mask_np = sample_mask.numpy()\n",
    "        median_np = median.numpy()\n",
    "        lower_np = lower.numpy()\n",
    "        upper_np = upper.numpy()\n",
    "\n",
    "        # ---- PLOTTING INTO ax ----\n",
    "        ax.scatter(\n",
    "            time_axis[obs_mask_np],\n",
    "            obs_series_np[obs_mask_np],\n",
    "            color=\"black\",\n",
    "            s=10,\n",
    "            label=\"Observed\",\n",
    "        )\n",
    "        ax.plot(time_axis, median_np, label=\"Median\", color=\"tab:green\")\n",
    "        ax.fill_between(\n",
    "            time_axis,\n",
    "            lower_np,\n",
    "            upper_np,\n",
    "            where=sample_mask_np,\n",
    "            alpha=0.3,\n",
    "            color=\"tab:green\",\n",
    "            label=\"10â€“90%\",\n",
    "        )\n",
    "\n",
    "        ax.set_title(f\"Feature {feat}\")\n",
    "        ax.set_xlabel(\"Time step\")\n",
    "        ax.set_ylabel(\"Value\")\n",
    "        ax.grid(True)\n",
    "        ax.legend()\n",
    "\n",
    "    # Turn off unused axes if any\n",
    "    for ax in axes[n_features:]:\n",
    "        ax.axis(\"off\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d9f255f",
   "metadata": {},
   "source": [
    "# CSDI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4e2d0bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from fits.modelling.CSDI.adapter import CSDIAdapter\n",
    "\n",
    "\n",
    "csdi = CSDIAdapter().to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9eb28f22",
   "metadata": {},
   "outputs": [],
   "source": [
    "Train(csdi, train_loader, valid_loader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4020462",
   "metadata": {},
   "outputs": [],
   "source": [
    "state = torch.load(\n",
    "    \"../data/models/training/CSDIAdapter_20251202_130631/model.pth\",\n",
    "    map_location=device,\n",
    ")\n",
    "\n",
    "csdi.load_state_dict(state)\n",
    "csdi.to(device)\n",
    "\n",
    "clear_output()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f7eec74",
   "metadata": {},
   "outputs": [],
   "source": [
    "Evaluate(csdi, test_loader, normalization_stats, nsample=5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a0411fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_forecast_sample(\"../data/models/evaluation/CSDIAdapter_20251202_161117\", nsample=5, sample_index=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1078b24a",
   "metadata": {},
   "source": [
    "# DiffusionTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d8418a38",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "unsupported operand type(s) for %: 'NoneType' and 'int'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 4\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mfits\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmodelling\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mDiffusionTS\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01madapter\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m DiffusionTSAdapter\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m diffusionts = \u001b[43mDiffusionTSAdapter\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m.to(device)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/courses/project/FITS/src/fits/modelling/DiffusionTS/adapter.py:68\u001b[39m, in \u001b[36mDiffusionTSAdapter.__init__\u001b[39m\u001b[34m(self, config)\u001b[39m\n\u001b[32m     66\u001b[39m \u001b[38;5;28msuper\u001b[39m().\u001b[34m__init__\u001b[39m(config)\n\u001b[32m     67\u001b[39m \u001b[38;5;28mself\u001b[39m.config: DiffusionTSConfig = config\n\u001b[32m---> \u001b[39m\u001b[32m68\u001b[39m \u001b[38;5;28mself\u001b[39m.diffusion = \u001b[43mDiffusion_TS\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdiffusion_kwargs\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m.to(\u001b[38;5;28mself\u001b[39m.device)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/courses/project/FITS/src/fits/modelling/DiffusionTS/interpretable_diffusion/gaussian_diffusion.py:65\u001b[39m, in \u001b[36mDiffusion_TS.__init__\u001b[39m\u001b[34m(self, seq_length, feature_size, n_layer_enc, n_layer_dec, d_model, timesteps, sampling_timesteps, loss_type, beta_schedule, n_heads, mlp_hidden_times, eta, attn_pd, resid_pd, kernel_size, padding_size, use_ff, reg_weight, **kwargs)\u001b[39m\n\u001b[32m     62\u001b[39m \u001b[38;5;28mself\u001b[39m.feature_size = feature_size\n\u001b[32m     63\u001b[39m \u001b[38;5;28mself\u001b[39m.ff_weight = default(reg_weight, math.sqrt(\u001b[38;5;28mself\u001b[39m.seq_length) / \u001b[32m5\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m65\u001b[39m \u001b[38;5;28mself\u001b[39m.model = \u001b[43mTransformer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn_feat\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfeature_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_channel\u001b[49m\u001b[43m=\u001b[49m\u001b[43mseq_length\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_layer_enc\u001b[49m\u001b[43m=\u001b[49m\u001b[43mn_layer_enc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_layer_dec\u001b[49m\u001b[43m=\u001b[49m\u001b[43mn_layer_dec\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     66\u001b[39m \u001b[43m                         \u001b[49m\u001b[43mn_heads\u001b[49m\u001b[43m=\u001b[49m\u001b[43mn_heads\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattn_pdrop\u001b[49m\u001b[43m=\u001b[49m\u001b[43mattn_pd\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mresid_pdrop\u001b[49m\u001b[43m=\u001b[49m\u001b[43mresid_pd\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmlp_hidden_times\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmlp_hidden_times\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     67\u001b[39m \u001b[43m                         \u001b[49m\u001b[43mmax_len\u001b[49m\u001b[43m=\u001b[49m\u001b[43mseq_length\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_embd\u001b[49m\u001b[43m=\u001b[49m\u001b[43md_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconv_params\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\u001b[43mkernel_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpadding_size\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     69\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m beta_schedule == \u001b[33m'\u001b[39m\u001b[33mlinear\u001b[39m\u001b[33m'\u001b[39m:\n\u001b[32m     70\u001b[39m     betas = linear_beta_schedule(timesteps)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/courses/project/FITS/src/fits/modelling/DiffusionTS/interpretable_diffusion/transformer.py:405\u001b[39m, in \u001b[36mTransformer.__init__\u001b[39m\u001b[34m(self, n_feat, n_channel, n_layer_enc, n_layer_dec, n_embd, n_heads, attn_pdrop, resid_pdrop, mlp_hidden_times, block_activate, max_len, conv_params, **kwargs)\u001b[39m\n\u001b[32m    388\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__init__\u001b[39m(\n\u001b[32m    389\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m    390\u001b[39m     n_feat,\n\u001b[32m   (...)\u001b[39m\u001b[32m    402\u001b[39m     **kwargs\n\u001b[32m    403\u001b[39m ):\n\u001b[32m    404\u001b[39m     \u001b[38;5;28msuper\u001b[39m().\u001b[34m__init__\u001b[39m()\n\u001b[32m--> \u001b[39m\u001b[32m405\u001b[39m     \u001b[38;5;28mself\u001b[39m.emb = \u001b[43mConv_MLP\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn_feat\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_embd\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mresid_pdrop\u001b[49m\u001b[43m=\u001b[49m\u001b[43mresid_pdrop\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    406\u001b[39m     \u001b[38;5;28mself\u001b[39m.inverse = Conv_MLP(n_embd, n_feat, resid_pdrop=resid_pdrop)\n\u001b[32m    408\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m conv_params \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m conv_params[\u001b[32m0\u001b[39m] \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/courses/project/FITS/src/fits/modelling/DiffusionTS/interpretable_diffusion/model_utils.py:228\u001b[39m, in \u001b[36mConv_MLP.__init__\u001b[39m\u001b[34m(self, in_dim, out_dim, resid_pdrop)\u001b[39m\n\u001b[32m    224\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, in_dim, out_dim, resid_pdrop=\u001b[32m0.\u001b[39m):\n\u001b[32m    225\u001b[39m     \u001b[38;5;28msuper\u001b[39m().\u001b[34m__init__\u001b[39m()\n\u001b[32m    226\u001b[39m     \u001b[38;5;28mself\u001b[39m.sequential = nn.Sequential(\n\u001b[32m    227\u001b[39m         Transpose(shape=(\u001b[32m1\u001b[39m, \u001b[32m2\u001b[39m)),\n\u001b[32m--> \u001b[39m\u001b[32m228\u001b[39m         \u001b[43mnn\u001b[49m\u001b[43m.\u001b[49m\u001b[43mConv1d\u001b[49m\u001b[43m(\u001b[49m\u001b[43min_dim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout_dim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m3\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstride\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpadding\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m)\u001b[49m,\n\u001b[32m    229\u001b[39m         nn.Dropout(p=resid_pdrop),\n\u001b[32m    230\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/courses/venv311/lib/python3.11/site-packages/torch/nn/modules/conv.py:338\u001b[39m, in \u001b[36mConv1d.__init__\u001b[39m\u001b[34m(self, in_channels, out_channels, kernel_size, stride, padding, dilation, groups, bias, padding_mode, device, dtype)\u001b[39m\n\u001b[32m    336\u001b[39m padding_ = padding \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(padding, \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m _single(padding)\n\u001b[32m    337\u001b[39m dilation_ = _single(dilation)\n\u001b[32m--> \u001b[39m\u001b[32m338\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[34;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[32m    339\u001b[39m \u001b[43m    \u001b[49m\u001b[43min_channels\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    340\u001b[39m \u001b[43m    \u001b[49m\u001b[43mout_channels\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    341\u001b[39m \u001b[43m    \u001b[49m\u001b[43mkernel_size_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    342\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstride_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    343\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpadding_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    344\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdilation_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    345\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    346\u001b[39m \u001b[43m    \u001b[49m\u001b[43m_single\u001b[49m\u001b[43m(\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    347\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgroups\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    348\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    349\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpadding_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    350\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mfactory_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    351\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/courses/venv311/lib/python3.11/site-packages/torch/nn/modules/conv.py:109\u001b[39m, in \u001b[36m_ConvNd.__init__\u001b[39m\u001b[34m(self, in_channels, out_channels, kernel_size, stride, padding, dilation, transposed, output_padding, groups, bias, padding_mode, device, dtype)\u001b[39m\n\u001b[32m    107\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m in_channels % groups != \u001b[32m0\u001b[39m:\n\u001b[32m    108\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33min_channels must be divisible by groups\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m109\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mout_channels\u001b[49m\u001b[43m \u001b[49m\u001b[43m%\u001b[49m\u001b[43m \u001b[49m\u001b[43mgroups\u001b[49m != \u001b[32m0\u001b[39m:\n\u001b[32m    110\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mout_channels must be divisible by groups\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    111\u001b[39m valid_padding_strings = {\u001b[33m\"\u001b[39m\u001b[33msame\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mvalid\u001b[39m\u001b[33m\"\u001b[39m}\n",
      "\u001b[31mTypeError\u001b[39m: unsupported operand type(s) for %: 'NoneType' and 'int'"
     ]
    }
   ],
   "source": [
    "from fits.modelling.DiffusionTS.adapter import DiffusionTSAdapter\n",
    "\n",
    "\n",
    "diffusionts = DiffusionTSAdapter().to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "482cde9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "Train(diffusionts, train_loader, valid_loader)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
